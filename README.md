Madison RL Agent â€“ Reinforcement Learning for Agentic AI Systems
Take-Home Final Project â€” LLMs & Agentic AI Systems

This project implements a reinforcement-learningâ€“enhanced version of the Madison Intelligence Agent Framework, integrating Q-learning and UCB bandit exploration to improve source selection, retrieval quality, and synthesized answer accuracy across 1000 training episodes.

The system demonstrates how agentic AI + reinforcement learning can optimize decision-making in multi-agent pipelines.

â­ 1. Project Overview

This project enhances the Madison agentic framework by adding:

Q-Learning (value-based reinforcement learning)

UCB (Upper Confidence Bound) for strategic exploration

Reward shaping using quality, verification, and alignment bonus

Agent pipeline including SearchAgent, Summarizer, Verifier, and Synthesizer

The result is an RL-powered system that improves its ability to answer queries by learning which information sources yield the highest reward.

ğŸ“ 2. System Architecture
User Query
     â†“
ControllerEnvironment (RL)
 â”œâ”€â”€ Q-Learning Agent
 â”œâ”€â”€ UCB Bandit
 â””â”€â”€ Query-Type Encoder
     â†“
Source Selection (0â€“4)
     â†“
SearchAgent â†’ Summarizer â†’ Verifier â†’ Synthesizer
     â†“
Final Answer + Reward
     â†“
RL Update (Q-table + Bandit)

ğŸ”¬ 3. Reinforcement Learning Components
Q-Learning Update Rule
ğ‘„
(
ğ‘ 
,
ğ‘
)
â†
ğ‘„
(
ğ‘ 
,
ğ‘
)
+
ğ›¼
[
ğ‘Ÿ
+
ğ›¾
max
â¡
ğ‘
â€²
ğ‘„
(
ğ‘ 
â€²
,
ğ‘
â€²
)
âˆ’
ğ‘„
(
ğ‘ 
,
ğ‘
)
]
Q(s,a)â†Q(s,a)+Î±[r+Î³
a
â€²
max
	â€‹

Q(s
â€²
,a
â€²
)âˆ’Q(s,a)]
UCB Exploration Strategy
ğ‘
=
arg
â¡
max
â¡
ğ‘–
(
ğ‘„
ğ‘–
+
2
ln
â¡
ğ‘
ğ‘›
ğ‘–
+
1
)
a=arg
i
max
	â€‹

(Q
i
	â€‹

+
n
i
	â€‹

+1
2lnN
	â€‹

	â€‹

)
Reward Function
ğ‘…
=
0.7
(
ğ‘
ğ‘¢
ğ‘
ğ‘™
ğ‘–
ğ‘¡
ğ‘¦
)
+
0.3
(
ğ‘£
ğ‘’
ğ‘Ÿ
ğ‘–
ğ‘“
ğ‘–
ğ‘
ğ‘
ğ‘¡
ğ‘–
ğ‘œ
ğ‘›
)
+
ğ‘
ğ‘™
ğ‘–
ğ‘”
ğ‘›
ğ‘š
ğ‘’
ğ‘›
ğ‘¡
_
ğ‘
ğ‘œ
ğ‘›
ğ‘¢
ğ‘ 
R=0.7(quality)+0.3(verification)+alignment_bonus

The alignment bonus rewards selecting the correct source for the query type (general, research, news).

ğŸ§ª 4. Experimental Setup

Episodes: 1000

Actions: 5 sources

Query types: general, research, news

Learning rate: 0.1

Discount: 0.9

Bandit: UCB

Environment: custom ControllerEnvironment

Each training episode includes:

State encoding

Source selection (Q-learning or bandit)

Retrieval â†’ summarization â†’ verification

Reward calculation

RL updates

ğŸ“Š 5. Results Summary
âœ” Learning Curve

Reward improves steadily and converges near 0.7â€“0.8 by episode ~700.

âœ” Source Preference Visualization

Agent learns stable preference patterns depending on query type.

âœ” Trained vs Untrained Comparison

The trained agent consistently outperforms the untrained baseline in:

Reward

Source accuracy

Behavioral consistency

Quality of generated answers

ğŸ–¼ï¸ 6. Visualizations

Generated in the notebook:

Learning Curve (Reward vs Episodes)

Source Preference Distribution (Q-table convergence)

Trained vs Untrained Bar Chart

These satisfy the assignment requirement for visual evaluation.

ğŸ“ 7. Repository Structure (Recommended)
madison-rl-agent/
â”‚
â”œâ”€â”€ main.ipynb                     # Full training + evaluation notebook
â”œâ”€â”€ controller_environment.py      # RL controller
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ search_agent.py
â”‚   â”œâ”€â”€ summarizer.py
â”‚   â”œâ”€â”€ verifier.py
â”‚   â”œâ”€â”€ evaluator.py
â”‚   â””â”€â”€ synthesizer.py
â”œâ”€â”€ rl/
â”‚   â”œâ”€â”€ q_learning_agent.py
â”‚   â””â”€â”€ bandit_ucb.py
â””â”€â”€ README.md

âš™ï¸ 8. How to Run
Install Dependencies
pip install numpy matplotlib tqdm

Train the Agent
env = ControllerEnvironment()
for ep in range(1000):
    query = random.choice(training_queries)
    env.run_episode(query, training=True)

Generate Visualizations
plot_learning_curve(env)
plot_source_preference_array(q_agent)
compare_trained_untrained(env)

ğŸ”® 9. Future Improvements

Replace Q-table with Deep Q-Network (DQN)

Apply policy gradient methods (PPO/A3C)

Integrate real web APIs or news feeds

Perform multi-agent reinforcement learning

Add memory-based reasoning

ğŸ›¡ï¸ 10. Ethical Considerations

RL may reinforce biased source selection

Misinformation risk if sources are poor-quality

Transparency needed in reward rationale

Behaviors must remain aligned with user safety

ğŸ 11. Conclusion

The Madison RL Agent demonstrates that reinforcement learning can significantly improve agentic system performance. Through Q-learning, UCB exploration, and reward shaping, the agent converges to stable, high-quality retrieval and decision policies.

This project fulfills all core requirements for the Reinforcement Learning Take-Home Final.
